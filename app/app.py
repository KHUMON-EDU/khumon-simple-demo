import os
import tempfile
import re

import streamlit as st
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import ChatOpenAI
from PIL import Image
import whisper

load_dotenv()

url_regex = (
    "(https:\/\/www\.|http:\/\/www\.|https:\/\/|http:\/\/)?[a-zA-Z0-9]{2,}(\.[a-zA-Z0-9]{2,})(\.[a-zA-Z0-9]{2,})?"
)
with st.spinner("ëª¨ë¸ ì¤€ë¹„ ì¤‘ ğŸƒ"):
    model = whisper.load_model("base")

def process_pdf(source):
    try:
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_file.write(source.read())
        loader = PyPDFLoader(tmp_file.name, extract_images=is_ocr)
        pages = loader.load_and_split()
        os.remove(tmp_file.name)
        docs = get_docs(pages)
        return docs

    except:
        pass

def process_mp4(source):
    try:
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_file.write(source.read())
        source_text = model.transcribe(tmp_file.name, verbose=True)['text']
        return source_text

    except:
        pass

def get_docs(pages):
    docs = ""
    for page in pages:
        content = page.page_content
        replace_content = re.sub(url_regex, "", content)
        docs += replace_content
    return docs


logo_image = Image.open("static/logo.jpg")
st.image(logo_image, width=100)
st.title("KHUMON DEMO")

with st.sidebar:
    mode = st.radio("Input Format",["PDF", "VIDEO"])

if mode == 'PDF':
    is_ocr = st.toggle('Extract images from pdf')
    source = st.file_uploader("ê°•ì˜ PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!", type="pdf", label_visibility="collapsed")

elif mode =="VIDEO":
    source = st.file_uploader("ê°•ì˜ ë¹„ë””ì˜¤(MP4)ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!", type="mp4", label_visibility="collapsed")



if st.button("Make! âœˆï¸"):
    try:
        with st.spinner("ë¶„ì„ ì¤‘ ğŸƒ"):
            if mode == 'PDF':
                docs = process_pdf(source)
            elif mode == 'VIDEO':
                docs = process_mp4(source)

            llm = ChatOpenAI(temperature=0)
            summary = llm.predict(
                f"ë‹¹ì‹ ì€ ì „ê³µ ê°•ì˜ ìë£Œ ìš”ì•½ê¸° ì…ë‹ˆë‹¤. ì•„ë˜ ì£¼ì–´ì§„ ëŒ€ë³¸ì„ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ì ì ˆí•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. ë§í¬ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´, ë¬¸ì¥ ë“¤ì€ ë¬´ì‹œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤. ëŒ€ë³¸: {docs[:10000]} "
            )
            question = llm.predict(
                f"ë‹¹ì‹ ì€ ì „ê³µ ê°•ì˜ë¥¼ ìš”ì•½í•œ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ë¬¸ì œë¥¼ 10ê°œë¥¼ ë§Œë“œì„¸ìš”. ë¬¸ì œëŠ” ë‹¨ë‹µí˜• ë˜ëŠ” ì£¼ê´€ì‹ìœ¼ë¡œ ë§Œë“œì„¸ìš”. ëŒ€ë³¸: {summary} "
            )
        st.subheader("âœ’ï¸ ìš”ì•½")
        st.text(summary)
        st.subheader("â“ ì§ˆë¬¸")
        st.text(question)
    except Exception as e:
        st.error(f"An error occurred: {e}")
